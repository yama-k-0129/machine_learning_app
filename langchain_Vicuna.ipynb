{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yama-k-0129/machine_learning_app/blob/main/langchain_Vicuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデルのダウンロード"
      ],
      "metadata": {
        "id": "HD9u6uv_-kQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのダウンロード\n",
        "!wget https://huggingface.co/TheBloke/vicuna-13B-v1.5-GGML/resolve/main/vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\n",
        "PATH_MODEL = \"/content/vicuna-13b-v1.5.ggmlv3.q5_K_M.bin\""
      ],
      "metadata": {
        "id": "8k2dQw7ecchX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxYvIkJVD8yD"
      },
      "source": [
        "# llama-cpp-pythonでモデルを使用する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifmWA0WUFMCC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6TeEyU3FMhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c171367-54cb-4449-f49a-984ea1d6b342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "# モデルのダウンロード\n",
        "import llama_cpp\n",
        "import ctypes\n",
        "\n",
        "llm =llama_cpp.Llama(model_path=PATH_MODEL, n_gpu_layers=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V--KIrMovJWt"
      },
      "outputs": [],
      "source": [
        "# ref:https://github.com/lm-sys/FastChat/blob/d578599c69d060e6d40943f1b5b72af98956092a/fastchat/conversation.py#L286-L299\n",
        "TEMPLATE = \"\"\"\n",
        "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "全て日本語で返答を行います。\n",
        "\n",
        "USER:TopYoutuberになるにはどうすればよいですか？\n",
        "ASSISTANT:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJzLNzaVGoq0",
        "outputId": "8650b839-cdbe-4e30-d800-881bb22b65e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 自分の強みや特技を見つけ、それを活かしたコンテンツを作成する。\n",
            "2. ユーザーとのコミュニケーションを大切にし、リアルな情報発信を行う。\n",
            "3. 他のYouTuberと競合しない独自の個性を持つこと。\n",
            "4. 定期的に動画を更新することで視聴者の注目度を高める。\n",
            "5. SEO対策を行って、検索エンジンで上位表示されるようにする。\n"
          ]
        }
      ],
      "source": [
        "# 推論の実行\n",
        "output = llm(\n",
        "    TEMPLATE,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "print(output[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Y7dIGlB_bQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAAKPQ2LQmmo"
      },
      "source": [
        "## langchainと組み合わせる"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "U80hr03MTU9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "773xsyVgTWGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UF22gR8LLeO"
      },
      "source": [
        "# Basic langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj1ZfsjGUjOB"
      },
      "outputs": [],
      "source": [
        "TEMPLATE = \"\"\"\n",
        "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "全て日本語で返答を行います。\n",
        "\n",
        "USER:{user_input}\n",
        "ASSISTANT:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(TEMPLATE)\n",
        "formatted_prompt = prompt.format(user_input=\"こんにちは\")\n",
        "\n",
        "response = llm(prompt=formatted_prompt, max_tokens=2048)\n",
        "print(response['choices'][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-_pCNSy_x4s",
        "outputId": "f9aa6c64-0e9f-4a49-a053-c4460079bcda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは！どのようなお話がしたいか教えてください。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqUOZqZzXQYB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBU_gzSjN__1"
      },
      "source": [
        "# Chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU メモリ解放\n",
        "import gc\n",
        "del llm\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm_eWiiBEhEf",
        "outputId": "4dff5753-a2c6-438f-bbf0-22fdf4edb0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVtiSHoVPMpg"
      },
      "outputs": [],
      "source": [
        "# https://python.langchain.com/docs/integrations/llms/llamacpp#gpu\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import LlamaCpp\n",
        "# from langchain.callbacks.manager import CallbackManager\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=PATH_MODEL,\n",
        "    n_gpu_layers=40,\n",
        "    n_batch=128,\n",
        "    max_tokens=4096,\n",
        "    n_ctx=4096,\n",
        "    # callback_manager=callback_manager, 何故か精度が落ちる\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHfBvNgpEpGl",
        "outputId": "37829deb-f2f3-497b-ee9d-c9c3b41cbffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqed7haQOACk"
      },
      "outputs": [],
      "source": [
        "TEMPLATE = \"\"\"\n",
        "{system_input}\n",
        "\n",
        "USER:{user_input}\n",
        "ASSISTANT:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GhqkzZbPXQ0"
      },
      "outputs": [],
      "source": [
        "# Chainの定義\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pxQtegPOAGV",
        "outputId": "c6c4a848-0ab7-4752-91d8-e467cba683aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "日本の首都は東京です。\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    chain.run(\n",
        "        {\"system_input\": \"あなたは親切な人工知能です。全て日本語で答えてください\", \"user_input\": \"日本の首都はどこでしょうか？\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKkJfgQ2Qlpq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEVn1Kc4osOx"
      },
      "source": [
        "# Summarization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU メモリ解放\n",
        "del chain\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDZ32fi5E8tX",
        "outputId": "2b91d8f4-b6ad-4109-8b5c-49e0cf10d2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain"
      ],
      "metadata": {
        "id": "jgSE4Qpn9-mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPLATE = \"\"\"\n",
        "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
        "全て日本語で返答を行います。\n",
        "\n",
        "USER:次の文章の簡潔な要約をしてください:{text}\n",
        "ASSISTANT:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sApQHW4kGR_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(TEMPLATE)"
      ],
      "metadata": {
        "id": "1dySXJc7GVHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Define StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
        ")"
      ],
      "metadata": {
        "id": "0hSqoKozGYrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader"
      ],
      "metadata": {
        "id": "Lz56TQ-CG5KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://note.com/nyanta123/n/ncd9dae432356\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "3e6xYRmbG3fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stuff_chain.run(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynP-xoh0TPOr",
        "outputId": "1054eb55-e57f-45d7-a04c-e8d4ee9d04f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "この記事では、ReazonSpeechという日本語に特化した音AIを使って文字起こしを行い、度を介します。ReazonSpeechは、ワンセグ放送の画データから学させた19,000時間のデータを持つモデルで、日本語に特化したモデルとしては世界最高度をります。ReazonSpeechの特としては、パラメータが少なくても高度であることがげられます。ReazonSpeechを使った文字起こしの方法はで、Googleコラボ上で実行することができます。文字起こししたい動画ファイルをアップロードして前理を行うだけで、ReazonSpeechのモデルや必要なパッケージをインストールして実行することができます。この記事は、ReazonSpeechの利用方法と度についてしく解されており、日本語に特化した音AIとしての活用法が知りたい人におすすめです。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJuIjEcIA45C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBkQRi9CVmAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gpt-3.5-turboで要約してみる"
      ],
      "metadata": {
        "id": "lPLegtLlArv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NeO96pQGyaJ",
        "outputId": "bb3a3bb3-607d-48b6-db53-2f697afb75b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "1YoS1xwpKxQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "yQXNvxDbXN8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\"), prompt=prompt)\n",
        "\n",
        "docs = loader.load()\n",
        "print(llm_chain.run(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mle1xq09GCu2",
        "outputId": "ecc1a984-c08c-4c46-9529-3887460c0a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "この文章は、日本語に特化した音声認識AI「ReazonSpeech」を使用して高精度の文字起こしを行った結果について紹介しています。ReazonSpeechは、株式会社レアゾン・ホールディングスが開発した音声認識に関するモデルやデータの総称であり、音声をテキストに変換するAIです。このモデルは、ワンセグ放送の録画データから学習されたもので、日本語に特化しており、世界最高精度の音声認識モデルとされています。また、ReazonSpeechは比較的低スペックな計算機でも動作することができます。具体的な使い方としては、Googleコラボ上で簡単に文字起こしを行うことができます。使用方法や感想、今後の課題についても記事で紹介されています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ASFQQhtQ86G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwCvt3PjLOwd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}